head(Groceries)
view(Groceries)
summary(Groceries)
g <- Groceries
g[1:5]
Groceries
?view
?aprioriViz
?apriori
itemFrequency(Groceries)
data(iris)
"iris"
iris
str(iris)
View(iris)
summary(iris)
Iris.features = iris
Iris.features
Iris.features$class <0 NULL
head(Iris.features)
Iris.features$class <- NULL
head(Iris.features)
Iris.features$Species <- NULL
head(Iris.features)
results <- kmeans(Iris.features, 3)
results
results$size
results$cluster
results$centers
results$totss
results$cluster
results$totss
?kmeans
table(Iris$Species, results$cluster)
table(Iris$class, results$cluster)
results$cluster
Iris
iris$Species
table(iris$class, results$cluster)
data("iris")
iris
data("iris")
str(iris)
summary(iris)
iris.features = iris
iris.features$Species <- NULL
iris.features$Species
iris.features
results <- kmeans(iris.features, 3)
results
results$size
table(iris$Species, results$cluster)
plot(iris[c("petal.length","petal.width")], col(results$cluster))
plot(iris[c("petal.length","petal.width")], col = results$cluster)
plot(iris[c("petal.length","petal.width")], col results$cluster)
?plot
plot(iris[c("petal.length","petal.width")], col = results$cluster)
iris[c]
iris[c("petal.length")]
iris[c(petal.length)]
head(iris)
plot(iris[c("Petal.Length","Petal.Width")], col = results$cluster)
plot(iris[c("Petal.Length","Petal.Width")], col = results$class)
plot(iris[c("Petal.Length","Petal.Width")], col = results$Species)
head(results)
plot(iris[c("Petal.Length","Petal.Width")], col = results$cluster)
plot(iris[c("Sepal.Length","Sepal.Width")], col = results$cluster)
plot(iris[c("Sepal.Length","Sepal.Width")], col = results$class)
plot(iris[c("Sepal.Length","Sepal.Width")], col = results$Species)
install(neuralnet)
install("neuralnet")
install.packages("neuralnet")
data("infert")
dim(infert)
summary(infert)
variables(infert)
head(infert)
?infert
?neuralnet
require(neuralnet)
?"neuralnet"
?sse
nn = neuralnet(case~age+parit+induced+spontaneous, data = infert, hidden = 2, err.fct = "ce", linear.output = FALSE)
nn = neuralnet(case~age+parity+induced+spontaneous, data = infert, hidden = 2, err.fct = "ce", linear.output = FALSE)
nn
plot(nn)
nn$covariate
?covariate
infert$case
nn$net.result[(1)]
nn1 = ifelse(nn$net.result[(1)]>0.5,1,0)
nn1 = ifelse(nn$net.result[(1)] > 0.5,1,0)
nn1 = ifelse(nn$net.result[(1)] > .5,1,0)
nn1 = ifelse(nn$net.result[(1)] > 0.5, 1, 0)
nn$net.result[(1)]
nn$net.result[,1]
nn1 = ifelse(nn$net.result[(,1)] > 0.5, 1, 0)
nn1 = ifelse(nn$net.result[,1] > 0.5, 1, 0)
nn$net.result[,1]
nn$net.result[(1)]
nn$net.result[[1]]
nn[,1]
nn
head(nn)
summary(nn)
variable.names(nn)
nn1
nn1 = ifelse(nn$net.result[[1]] > 0.5, 1, 0)
nn1
data("iris")
str(iris)
iris.features = iris
iris.features$Species <- NULL
str(iris.features)
result <- kmeans(iris.features, 3)
result
result$cluster
result$centers
result$size
table(iris$Species, result$cluster)
plot(iris[c("Petal.Length", "Petal.Width")], col = result$cluster)
plot(iris[c("Sepal.Length", "Sepal.Width")], col = result$cluster)
sales <- matrix(c(11,34,56, NA, 78, NA, 91, 11, 1112, NA, NA, 1516), ncol = 3, byrow = TRUE)
sales
colnames(sales) <- c("A", "B", "C")
rownames(sales) <- c("Q1", "Q2", "Q3", "Q4")
sales
for (i 1:ncol(sales)) {}
for (i 1:ncol(sales)) {sales[is.na[,i]), i] <- mean(sales[,i], na.rm = TRUE)}
for (i in 1:ncol(sales)) {sales[is.na[,i]), i] <- mean(sales[,i], na.rm = TRUE)}
for (i in 1:ncol(sales)) {sales[is.na[,i]), i] <- mean(sales[,i], na.rm = TRUE}
for (i in 1:ncol(sales) {sales[is.na[,i]), i] <- mean(sales[,i], na.rm = TRUE}
for (i in 1:ncol(sales)) {sales[is.na[,i], i] <- mean(sales[,i], na.rm = TRUE}
for (i in 1:ncol(sales)) {sales[is.na[,i], i] <- mean(sales[,i], na.rm = TRUE)}
for (i in 1:ncol(sales)) {sales[is.na(sales[,i], i] <- mean(sales[,i], na.rm = TRUE)}
for (i in 1:ncol(sales)) {sales[is.na(sales[,i]), i] <- mean(sales[,i], na.rm = TRUE)}
sales
install.packages("neuralnet")
require(neuralnet)
data(infert)
infert
dim(infert)
head(infert)
?infert
nn = neuralnet(case~age+parity+spontaneous, data = infert, hidden = 2, err.fct = "ce", linear.output = FALSE)
nn
plot(nn)
install.packages("tm")
load(tm)
load("tm")
library(tm)
install.packages("devtools")
devtools::install_github("twitter/AnomalyDetection")
install.packages("devtools")
?devtools
library(AnomalyDetection)
?AnomalyDetection
?AnomalyDetectionTs
devtools::install_github("twitter/AnomalyDetection")
data(raw_data)
install.packages('tm')
install.packages('SnowballC')
require('tm')
require('SnowballC')
folderdir = '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(William Ford)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/20 Newsgroups'
folderdir1 = '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(William Ford)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/20 Newsgroups/20news-bydate'
folderdir2 = '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(William Ford)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/20 Newsgroups/20news-bydate/20news-bydate-test/sci.electronics'
sci.electr.train <- Corpus(DirSource(folderdir2, encoding = "UTF-8"), rederControl=list(reader=readPlain, language = "en"))
sci.electr.train <- Corpus(DirSource(folderdir2, encoding = "UTF-8"), readerControl=list(reader=readPlain, language = "en"))
#initialize
libs <- c("tm", "plyr", "class")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = FALSE)
candidates <- c("bush_gw","obama_gw")
pathname <- '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/speeches'
#Clean the text
cleanCorpus <- function(corpus) {
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords(kind = "english"))
return(corpus.tmp)
}
#Build a candidate matrix for two candidates and attach name
generateTDM <- function(cand, path) {
s.dir <- sprintf("%s/%s", path, cand)
#for Mac, use 'UTF-8', for Windows, use 'ANSI'
s.cor <- Corpus(DirSource(directory = s.dir, encoding = 'UTF-8'))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.7)
result <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path = pathname)
candidates <- c("bush_gw","obama_bh")
pathname <- '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/speeches'
libs <- c("tm", "plyr", "class")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = FALSE)
candidates <- c("bush_gw","obama_bh")
pathname <- '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/speeches'
#Clean the text
cleanCorpus <- function(corpus) {
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords(kind = "english"))
return(corpus.tmp)
}
#Build a candidate matrix for two candidates and attach name
generateTDM <- function(cand, path) {
s.dir <- sprintf("%s/%s", path, cand)
#for Mac, use 'UTF-8', for Windows, use 'ANSI'
s.cor <- Corpus(DirSource(directory = s.dir, encoding = 'UTF-8'))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.7)
result <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path = pathname)
?content_transformer
#Clean the text
cleanCorpus <- function(corpus) {
corpus.tmp <- tm_map(corpus, content_transformer(removePunctuation))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(stripWhitespace))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(removeWords), stopwords(kind = "english"))
return(corpus.tmp)
}
#Build a candidate matrix for two candidates and attach name
generateTDM <- function(cand, path) {
s.dir <- sprintf("%s/%s", path, cand)
#for Mac, use 'UTF-8', for Windows, use 'ANSI'
s.cor <- Corpus(DirSource(directory = s.dir, encoding = 'UTF-8'))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.7)
result <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path = pathname)
warnings()
str(tdm)
bindCandidateToTDM <- function(tdm) {
s.mat <- t(data.matrix(tdm[["tdm"]]))
s.df <- as.data.frame(s.mat, stringsAsFactors = FALSE)
s.df <- cbind(s.df, rep(tdm[["name"]], nrow(s.df)))
colnames(s.df)[ncol(s.df)]
return(s.df)
}
candTDM <- lapply(tdm, bindCandidateToTDM)
str(candTDM)
setwd('/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150728-Anomaly Detection and Text Mining/Text Mining/speeches')
getwd()
getwd()
cat('\014')
libs <- c('tm', 'plyr', 'class')
lapply(libs, require, character.only=TRUE)
optinos(stringsAsFactors = FALSE)
options(stringsAsFactors = FALSE)
candidates <- ("bush_gw, "obama_bh)
candidates <- ("bush_gw", "obama_bh")
candidates <- ("bush_gw","obama_bh")
candidates <- c("bush_gw","obama_bh")
pathname <- "/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150728-Anomaly Detection and Text Mining/Text Mining/speeches"
cleanCorpus <- function(corpus) {
corpus.tmp <- tm_map(corpus, content_transformer(removePunctuation))
corpus.tmp <- tm_map(corpus, content_transformer(stripWhitespace))
corpus.tmp <- tm_map(corpus, content_transformer(tolower))
corpus.tmp <- tm_map(corpus, content_transformer(removeWords), stopwords("english"))
return(corup.tmp)
}
generateTDM <- function(cand, path) {
s.dir <- sprintf("%s/%s", path, cand)
s.cor <- Corpus(DirSource(directory = s.dir, encoding = "UTF-8"))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.7)
return <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path=pathname)
cleanCorpus <- function(corpus) {
}
generateTDM <- function(cand, path) {
s.dir <- sprintf("%s/%s", path, cand)
#for Mac, use 'UTF-8', for Windows, use 'ANSI'
s.cor <- Corpus(DirSource(directory = s.dir, encoding = 'UTF-8'))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.7)
result <- list(name = cand, tdm = s.tdm)
}
cleanCorpus <- function(corpus) {
corpus.tmp <- tm_map(corpus, content_transformer(removePunctuation))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(stripWhitespace))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(removeWords), stopwords(kind = "english"))
return(corpus.tmp)
}
tdm <- lapply(candidates, generateTDM, path=pathname)
str(tdm)
bindCandidateToTDM <- function(tdm) {
s.mat <- t(data.matrix(tdm[['tdm']])
s.df <- as.data.frame(s.mat, stringsAsFactors = FALSE)
s.mat <- t(data.matrix(tdm[['tdm']]))
s.mat <- t(data.matrix(tdm[['tdm']]))
s.df <- as.data.frame(s.mat, stringsAsFactors = Flase)
s.df <- as.data.frame(s.mat, stringsAsFactors = False)
s.mat
?data.matrix
s.mat <- t(data.matrix(tdm[["tdm"]]))
str(tdm)
#initialize
libs <- c("tm", "plyr", "class")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = FALSE)
candidates <- c("bush_gw","obama_bh")
pathname <- '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/speeches'
#Clean the text
cleanCorpus <- function(corpus) {
corpus.tmp <- tm_map(corpus, content_transformer(removePunctuation))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(stripWhitespace))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(removeWords), stopwords(kind = "english"))
return(corpus.tmp)
}
#Build a candidate matrix for two candidates and attach name
generateTDM <- function(cand, path) {
s.dir <- sprintf("%s/%s", path, cand)
#for Mac, use 'UTF-8', for Windows, use 'ANSI'
s.cor <- Corpus(DirSource(directory = s.dir, encoding = 'UTF-8'))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.7)
result <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path = pathname)
#show the results
str(tdm)
?lapply
libs <- c("tm", "plyr", "class")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = FALSE)
candidates <- c("bush_gw","obama_bh")
pathname <- '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/speeches'
#Clean the text
cleanCorpus <- function(corpus) {
corpus.tmp <- tm_map(corpus, content_transformer(removePunctuation))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(stripWhitespace))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(removeWords), stopwords(kind = "english"))
return(corpus.tmp)
}
#Build a candidate matrix for two candidates and attach name
generateTDM <- function(cand, path) {
s.dir <- sprintf("%s/%s", path, cand)
#for Mac, use 'UTF-8', for Windows, use 'ANSI'
s.cor <- Corpus(DirSource(directory = s.dir, encoding = 'UTF-8'))
s.dir <- sprintf("%s/%s", path, cand)
}
generateTDM <- function(cand, path) {
s.dir <- sprintf("%s/%s", path, cand)
#for Mac, use 'UTF-8', for Windows, use 'ANSI'
s.cor <- Corpus(DirSource(directory = s.dir, encoding = 'UTF-8'))
}
generateTDM <- function(cand, path) {
s.dir <- sprintf("%s/%s", path, cand)
#for Mac, use 'UTF-8', for Windows, use 'ANSI'
s.cor <- Corpus(DirSource(directory = s.dir, encoding = 'ANSI'))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.7)
result <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path = pathname)
char('014')
cat('\014')
library(tm)
library(SnowballC)
?SnowballC
library(qdap)
library(qdapDirectionaries)
library(dplyr)
library(plyr)
options(stringsAsFactors = FALSE)
s.dir = "/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150728-Anomaly Detection and Text Mining/Text Mining/speeches/obama_bh"
s.cor <- Corpus(DirSource(directory = s.dir, encoding = 'UTF-8'))
class(s.cor)
class(s.cor[[1]])
summary(s.dir)
inspect(s.cor)
getTransformations()
cleanCorpus <- content_transformer(s.cor, toSpace, "/")
library(tm)
setwd("~/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150728-Anomaly Detection and Text Mining/Text Mining/speeches/obama_bh")
docs <- Corpus(DirSource(getwd()))
class(docs)
class(docs[[1]])
summary(docs)
getTransformations()
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, content, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(removePunctuation))
docs.tmp <- tm_map(docs, content_transformer(removePunctuation))
?tm_map
docs <- tm_map(docs, toSpace, tolower)
docs.tmp <- tm_map(docs, content_transformer(removePunctuation))
docs <- tm_map(docs, content_transformer(tolower))
install.packages('SnowballC')
install.packages("SnowballC")
docs <- tm_map(docs, content_transformer(tolower))
install.packages("tm")
install.packages("tm")
install.packages("tm")
install.packages("tm")
install.packages("tm")
libs <- c("tm", "plyr", "class", "SnowballC")
#set your working directory using setwd('[path]') or going to Session > Set Working Directory > Choose Directory
#create a Corpus
libs <- c("tm", "plyr", "class", "SnowballC")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = TRUE)
docs <- Corpus(DirSource(getwd()))
#create a function for removing special characters
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#apply a function for removing special characters
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
#further cleanup
docs <- tm_map(docs, content_transformer(removePunctuation))
docs <- tm_map(docs, content_transformer(stripWhitespace))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(removewords), stopwords("english"))
docs <- tm_map(docs, content_transformer(removePunctuation))
docs <- tm_map(docs, content_transformer(stripWhitespace))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(removeNumbers))
docs <- tm_map(docs, removewords, stopwords("english"))
libs <- c("tm", "plyr", "class", "SnowballC")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = TRUE)
#set your working directory using setwd('[path]') or going to Session > Set Working Directory > Choose Directory
#create a Corpus
docs <- Corpus(DirSource(getwd()))
#create a function for removing special characters
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#apply a function for removing special characters
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
#further cleanup
docs <- tm_map(docs, content_transformer(removePunctuation))
docs <- tm_map(docs, content_transformer(stripWhitespace))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(removeNumbers))
docs <- tm_map(docs, removewords, stopwords("english"))
libs <- c("tm", "plyr", "class", "SnowballC")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = TRUE)
#set your working directory using setwd('[path]') or going to Session > Set Working Directory > Choose Directory
#create a Corpus
docs <- Corpus(DirSource(getwd()))
#create a function for removing special characters
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#apply a function for removing special characters
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
#further cleanup
docs <- tm_map(docs, content_transformer(removePunctuation))
docs <- tm_map(docs, content_transformer(stripWhitespace))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(removeNumbers))
docs <- tm_map(docs, removeWords, stopwords("english"))
?stripWhitespace
data("crude")
crude[[1]]
stripWhitespace(crude[[1]])
libs <- c("tm", "plyr", "class", "SnowballC")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = TRUE)
#set your working directory using setwd('[path]') or going to Session > Set Working Directory > Choose Directory
#create a Corpus
docs <- Corpus(DirSource(getwd()))
#create a function for removing special characters
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#apply a function for removing special characters
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
#further cleanup
docs <- tm_map(docs, content_transformer(removePunctuation))
docs <- tm_map(docs, content_transformer(stripWhitespace))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(removeNumbers))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument)
#Create a Document Term Matrix
tdm <- TermDocumentMatrix(docs)
#See the term frequencies
freq <- colSums(as.matrix(tdm))
ord <- order(freq)
freq[head(ord)]
#most frequent terms
freq[tail(ord)]
#frequency of frequencies
head(table(freq), 15)
tail(table(freq), 15)
