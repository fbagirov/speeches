install.packages(rpart.plot)
install.packages('rpart.plot')
data(iris)
str(iris)
table(iris$Species)
head (iris)
set.seed(9850)
g <- runif(nrow(iris))
irisr <- iris[order(g),]
str(irisr)
library(rpart)
m3 <- rpart(Species ~ ., data = irisr[1:100,], method="class")
rpart.plot(m3)
p3 <- predict(m3, irisr[101:150,], type="class")
table(irisr[101:150,5], predicted=p3)
install.packages("randomForest")
library(randomForest)
data("iris")
set.seed(9850)
g <- runif(nrow(iris))
irisr <- iris[order(g),]
iris.traing <- iris[1:100,]
iris.test <- iris[101:150]
iris.test = iris[1:100]
iris.test <- iris[1:100]
iris.training <- iris[1:100]
iris.training = iris[1:100]
head(iris[1:100])
library(arules)
library(datasets)
data("Groceries")
head(Groceries)
summary(Groceries)
rules <- apriori(Groceries, parameter = list(supp=0.01, conf = 0.8))
options(digits = 2)
inspect(rules[1:5])
read(rules)
read(rules[1:5],)
read(rules[1:5])
iris(rules[1:5])
rules
rules <- apriori(Groceries)
inspect(rules[1:5])
rules[1:5]
rules <- apriori(Groceries, parameter = list(supp=0.01, conf=0.8))
inspect(rules[1:5])
summary(rules)
rules <- apriori(Groceries, parameter = list(0.01, 0.8))
summary(rules)
?apriori
rules <- apriori(Groceries, parameter = list(supp=0.1, conf=0.8))
summary(rules)
rules <- apriori(Groceries, parameter = list(supp=0.5, conf=0.8))
summary(rules)
rules <- apriori(Groceries, parameter = list(supp=0.01, conf=0.5))
summary(rules)
rules <- sort(rules, by="confidence", decreasing = T)
summary(rules)
inspect(rules)
inspect(rules[1:5])
rules <- apriori(data = Groceries, parameter = list(supp=0.01, conf=0.5), appearance = list(default="lhs", rhs="whole milk"), control = list(verbose=F))
)
rules <- apriori(data = Groceries, parameter = list(supp=0.01, conf=0.5), appearance = list(default="lhs", rhs="whole milk"), control = list(verbose=F))
rules <- sort(rules, decreasing = TRUE, by="confidence")
inspect(rules[1:5])
rules <- apriori(data = Groceries, parameter = list(supp=0.01, conf=0.5), appearance = list(default="lhs", rhs="whole milk"))
inspect(rules[1:5])
Groceries[1:5]
head(Groceries)
view(Groceries)
summary(Groceries)
g <- Groceries
g[1:5]
Groceries
?view
?aprioriViz
?apriori
itemFrequency(Groceries)
data(iris)
"iris"
iris
str(iris)
View(iris)
summary(iris)
Iris.features = iris
Iris.features
Iris.features$class <0 NULL
head(Iris.features)
Iris.features$class <- NULL
head(Iris.features)
Iris.features$Species <- NULL
head(Iris.features)
results <- kmeans(Iris.features, 3)
results
results$size
results$cluster
results$centers
results$totss
results$cluster
results$totss
?kmeans
table(Iris$Species, results$cluster)
table(Iris$class, results$cluster)
results$cluster
Iris
iris$Species
table(iris$class, results$cluster)
data("iris")
iris
data("iris")
str(iris)
summary(iris)
iris.features = iris
iris.features$Species <- NULL
iris.features$Species
iris.features
results <- kmeans(iris.features, 3)
results
results$size
table(iris$Species, results$cluster)
plot(iris[c("petal.length","petal.width")], col(results$cluster))
plot(iris[c("petal.length","petal.width")], col = results$cluster)
plot(iris[c("petal.length","petal.width")], col results$cluster)
?plot
plot(iris[c("petal.length","petal.width")], col = results$cluster)
iris[c]
iris[c("petal.length")]
iris[c(petal.length)]
head(iris)
plot(iris[c("Petal.Length","Petal.Width")], col = results$cluster)
plot(iris[c("Petal.Length","Petal.Width")], col = results$class)
plot(iris[c("Petal.Length","Petal.Width")], col = results$Species)
head(results)
plot(iris[c("Petal.Length","Petal.Width")], col = results$cluster)
plot(iris[c("Sepal.Length","Sepal.Width")], col = results$cluster)
plot(iris[c("Sepal.Length","Sepal.Width")], col = results$class)
plot(iris[c("Sepal.Length","Sepal.Width")], col = results$Species)
install(neuralnet)
install("neuralnet")
install.packages("neuralnet")
data("infert")
dim(infert)
summary(infert)
variables(infert)
head(infert)
?infert
?neuralnet
require(neuralnet)
?"neuralnet"
?sse
nn = neuralnet(case~age+parit+induced+spontaneous, data = infert, hidden = 2, err.fct = "ce", linear.output = FALSE)
nn = neuralnet(case~age+parity+induced+spontaneous, data = infert, hidden = 2, err.fct = "ce", linear.output = FALSE)
nn
plot(nn)
nn$covariate
?covariate
infert$case
nn$net.result[(1)]
nn1 = ifelse(nn$net.result[(1)]>0.5,1,0)
nn1 = ifelse(nn$net.result[(1)] > 0.5,1,0)
nn1 = ifelse(nn$net.result[(1)] > .5,1,0)
nn1 = ifelse(nn$net.result[(1)] > 0.5, 1, 0)
nn$net.result[(1)]
nn$net.result[,1]
nn1 = ifelse(nn$net.result[(,1)] > 0.5, 1, 0)
nn1 = ifelse(nn$net.result[,1] > 0.5, 1, 0)
nn$net.result[,1]
nn$net.result[(1)]
nn$net.result[[1]]
nn[,1]
nn
head(nn)
summary(nn)
variable.names(nn)
nn1
nn1 = ifelse(nn$net.result[[1]] > 0.5, 1, 0)
nn1
data("iris")
str(iris)
iris.features = iris
iris.features$Species <- NULL
str(iris.features)
result <- kmeans(iris.features, 3)
result
result$cluster
result$centers
result$size
table(iris$Species, result$cluster)
plot(iris[c("Petal.Length", "Petal.Width")], col = result$cluster)
plot(iris[c("Sepal.Length", "Sepal.Width")], col = result$cluster)
sales <- matrix(c(11,34,56, NA, 78, NA, 91, 11, 1112, NA, NA, 1516), ncol = 3, byrow = TRUE)
sales
colnames(sales) <- c("A", "B", "C")
rownames(sales) <- c("Q1", "Q2", "Q3", "Q4")
sales
for (i 1:ncol(sales)) {}
for (i 1:ncol(sales)) {sales[is.na[,i]), i] <- mean(sales[,i], na.rm = TRUE)}
for (i in 1:ncol(sales)) {sales[is.na[,i]), i] <- mean(sales[,i], na.rm = TRUE)}
for (i in 1:ncol(sales)) {sales[is.na[,i]), i] <- mean(sales[,i], na.rm = TRUE}
for (i in 1:ncol(sales) {sales[is.na[,i]), i] <- mean(sales[,i], na.rm = TRUE}
for (i in 1:ncol(sales)) {sales[is.na[,i], i] <- mean(sales[,i], na.rm = TRUE}
for (i in 1:ncol(sales)) {sales[is.na[,i], i] <- mean(sales[,i], na.rm = TRUE)}
for (i in 1:ncol(sales)) {sales[is.na(sales[,i], i] <- mean(sales[,i], na.rm = TRUE)}
for (i in 1:ncol(sales)) {sales[is.na(sales[,i]), i] <- mean(sales[,i], na.rm = TRUE)}
sales
install.packages("neuralnet")
require(neuralnet)
data(infert)
infert
dim(infert)
head(infert)
?infert
nn = neuralnet(case~age+parity+spontaneous, data = infert, hidden = 2, err.fct = "ce", linear.output = FALSE)
nn
plot(nn)
install.packages("tm")
load(tm)
load("tm")
library(tm)
install.packages("devtools")
devtools::install_github("twitter/AnomalyDetection")
install.packages("devtools")
?devtools
library(AnomalyDetection)
?AnomalyDetection
?AnomalyDetectionTs
devtools::install_github("twitter/AnomalyDetection")
data(raw_data)
install.packages('tm')
install.packages('SnowballC')
require('tm')
require('SnowballC')
folderdir = '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(William Ford)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/20 Newsgroups'
folderdir1 = '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(William Ford)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/20 Newsgroups/20news-bydate'
folderdir2 = '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(William Ford)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/20 Newsgroups/20news-bydate/20news-bydate-test/sci.electronics'
sci.electr.train <- Corpus(DirSource(folderdir2, encoding = "UTF-8"), rederControl=list(reader=readPlain, language = "en"))
sci.electr.train <- Corpus(DirSource(folderdir2, encoding = "UTF-8"), readerControl=list(reader=readPlain, language = "en"))
#initialize
libs <- c("tm", "plyr", "class")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = FALSE)
candidates <- c("bush_gw","obama_gw")
pathname <- '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/speeches'
#Clean the text
cleanCorpus <- function(corpus) {
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords(kind = "english"))
return(corpus.tmp)
}
#Build a candidate matrix for two candidates and attach name
generateTDM <- function(cand, path) {
s.dir <- sprintf("%s/%s", path, cand)
#for Mac, use 'UTF-8', for Windows, use 'ANSI'
s.cor <- Corpus(DirSource(directory = s.dir, encoding = 'UTF-8'))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.7)
result <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path = pathname)
candidates <- c("bush_gw","obama_bh")
pathname <- '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/speeches'
libs <- c("tm", "plyr", "class")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = FALSE)
candidates <- c("bush_gw","obama_bh")
pathname <- '/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150724-Anomaly Detection and Text Mining/Text Mining/speeches'
#Clean the text
cleanCorpus <- function(corpus) {
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords(kind = "english"))
return(corpus.tmp)
}
#Build a candidate matrix for two candidates and attach name
generateTDM <- function(cand, path) {
s.dir <- sprintf("%s/%s", path, cand)
#for Mac, use 'UTF-8', for Windows, use 'ANSI'
s.cor <- Corpus(DirSource(directory = s.dir, encoding = 'UTF-8'))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.7)
result <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path = pathname)
?content_transformer
#Clean the text
cleanCorpus <- function(corpus) {
corpus.tmp <- tm_map(corpus, content_transformer(removePunctuation))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(stripWhitespace))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
corpus.tmp <- tm_map(corpus.tmp, content_transformer(removeWords), stopwords(kind = "english"))
return(corpus.tmp)
}
#Build a candidate matrix for two candidates and attach name
generateTDM <- function(cand, path) {
s.dir <- sprintf("%s/%s", path, cand)
#for Mac, use 'UTF-8', for Windows, use 'ANSI'
s.cor <- Corpus(DirSource(directory = s.dir, encoding = 'UTF-8'))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.7)
result <- list(name = cand, tdm = s.tdm)
}
tdm <- lapply(candidates, generateTDM, path = pathname)
warnings()
str(tdm)
bindCandidateToTDM <- function(tdm) {
s.mat <- t(data.matrix(tdm[["tdm"]]))
s.df <- as.data.frame(s.mat, stringsAsFactors = FALSE)
s.df <- cbind(s.df, rep(tdm[["name"]], nrow(s.df)))
colnames(s.df)[ncol(s.df)]
return(s.df)
}
candTDM <- lapply(tdm, bindCandidateToTDM)
str(candTDM)
libs <- c("tm", "plyr", "class", "wordcloud", "SnowballC")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = TRUE)
#set your working directory using setwd('[path]') or going to Session > Set Working Directory > Choose Directory
#create a Corpus
docs <- Corpus(DirSource(getwd()))
#create a function for removing special characters
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#apply a function for removing special characters
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
#further cleanup
docs <- tm_map(docs, content_transformer(removePunctuation))
docs <- tm_map(docs, content_transformer(stripWhitespace))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(removeNumbers))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument)
worldcloud(docs, scale=c(5,0,5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.docs=FALSE, colors=brewer.pal(8,"Dark2"))
install.packages('wordcloud')
install.packages("wordcloud")
worldcloud(docs, scale=c(5,0,5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.docs=FALSE, colors=brewer.pal(8,"Dark2"))
?wordCloud
library(wordcloud)
worldcloud(docs, scale=c(5,0,5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.docs=FALSE, colors=brewer.pal(8,"Dark2"))
worldcloud(docs, scale=c(5,0,5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.docs=FALSE, colors=brewer.pal(8,"Dark2"))
wordcloud(docs, scale=c(5,0,5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.docs=FALSE, colors=brewer.pal(8,"Dark2"))
libs <- c("tm", "plyr", "class", "SnowballC")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = TRUE)
#set your working directory using setwd('[path]') or going to Session > Set Working Directory > Choose Directory
#create a Corpus
setwd("~/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150728-Anomaly Detection and Text Mining/Text Mining/speeches")
setwd("~/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150728-Anomaly Detection and Text Mining/Text Mining/speeches/obama_bh")
docs <- Corpus(DirSource(getwd()))
#create a function for removing special characters
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#apply a function for removing special characters
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
#further cleanup
docs <- tm_map(docs, content_transformer(removePunctuation))
docs <- tm_map(docs, content_transformer(stripWhitespace))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(removeNumbers))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument)
#Create a Document Term Matrix
tdm <- TermDocumentMatrix(docs)
#See the term frequencies
freq <- colSums(as.matrix(tdm))
ord <- order(freq)
freq[head(ord)]
#most frequent terms
freq[tail(ord)]
libs <- c("tm", "plyr", "class", "wordcloud", "SnowballC")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = TRUE)
#set your working directory using setwd('[path]') or going to Session > Set Working Directory > Choose Directory
#create a Corpus
docs <- Corpus(DirSource(getwd()))
#create a function for removing special characters
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#apply a function for removing special characters
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
#further cleanup
docs <- tm_map(docs, content_transformer(removePunctuation))
docs <- tm_map(docs, content_transformer(stripWhitespace))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(removeNumbers))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument)
#build a wordcloud
wordcloud(docs, scale=c(5,0,5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.docs=FALSE, colors=brewer.pal(8,"Dark2"))
?TermDocumentMatrix
tdm <- DocumentTermMatrix(docs)
#See the term frequencies
freq <- colSums(as.matrix(tdm))
ord <- order(freq)
freq[head(ord)]
#most frequent terms
#See the term frequencies
freq <- colSums(as.matrix(tdm))
ord <- order(freq)
freq[head(ord)]
#most frequent terms
freq[tail(ord)]
#frequency of frequencies
head(table(freq), 15)
tail(table(freq), 15)
#Create a Document Term Matrix
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
#See the term frequencies
freq <- colSums(as.matrix(tdm))
ord <- order(freq)
freq[head(ord)]
#most frequent terms
freq[tail(ord)]
#frequency of frequencies
head(table(freq), 15)
tail(table(freq), 15)
str(tdm)
str(dtm)
?DocumentTermMatrix
libs <- c("tm", "plyr", "class", "SnowballC")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = TRUE)
setwd('/Users/feyzi/Dropbox/03_1-Online Teaching/2-UMUC/2015-2Summer/DATA630-9020-20150518-20150809-2190-(Les Pang)/WEBEX/20150728-Anomaly Detection and Text Mining/Text Mining/speeches/obama_bh')
getwd
getwd()
docs <- Corpus(DirSource(getwd()))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#apply a function for removing special characters
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
#further cleanup
docs <- tm_map(docs, content_transformer(removePunctuation))
docs <- tm_map(docs, content_transformer(stripWhitespace))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(removeNumbers))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument)
?DocumentTermMatrix
?TermDocumentMatrix
#Create a Document Term Matrix
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
#See the term frequencies
freq <- colSums(as.matrix(tdm))
ord <- order(freq)
freq[head(ord)]
#See the term frequencies
freq <- colSums(as.matrix(dtm))
ord <- order(freq)
freq[head(ord)]
#most frequent terms
freq[tail(ord)]
#frequency of frequencies
head(table(freq), 15)
tail(table(freq), 15)
str(tdm)
str(dtm)
libs <- c("tm", "plyr", "class", "wordcloud", "SnowballC")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = TRUE)
#set your working directory using setwd('[path]') or going to Session > Set Working Directory > Choose Directory
#create a Corpus
docs <- Corpus(DirSource(getwd()))
#create a function for removing special characters
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#apply a function for removing special characters
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
#further cleanup
docs <- tm_map(docs, content_transformer(removePunctuation))
docs <- tm_map(docs, content_transformer(stripWhitespace))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(removeNumbers))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument)
#build a wordcloud
wordcloud(docs, scale=c(5,0,5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.docs=FALSE, colors=brewer.pal(8,"Dark2"))
libs <- c("tm", "plyr", "class", "SnowballC")
lapply(libs, require, character.only=TRUE)
options(stringsAsFactors = TRUE)
#set your working directory using setwd('[path]') or going to Session > Set Working Directory > Choose Directory
#create a Corpus
docs <- Corpus(DirSource(getwd()))
#create a function for removing special characters
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#apply a function for removing special characters
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
#further cleanup
docs <- tm_map(docs, content_transformer(removePunctuation))
docs <- tm_map(docs, content_transformer(stripWhitespace))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, content_transformer(removeNumbers))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument)
#Create a Document Term Matrix
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
?order
?frequency
